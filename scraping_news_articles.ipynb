{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping News Aritcles - Hindi Newspaper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Scraping newspaper article from a newpaper website. A program that will navigate thorught the section of the newspaper and there pages to scrap news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import time\n",
    "import urllib\n",
    "import requests\n",
    "import bs4\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jagaran Newspaper's official url\n",
    "url = \"https://www.jagran.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to request a link\n",
    "def request_url(link):\n",
    "    \"\"\"\n",
    "    It takes a url and returns the html as string.\n",
    "    \"\"\"\n",
    "    ## Slow things down ## \n",
    "    ## Let the site breath ##\n",
    "    time.sleep(2)\n",
    "    \n",
    "    response = requests.get(link)\n",
    "    html = response.text\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse html\n",
    "def parse_html(to_parse):\n",
    "    \"\"\"\n",
    "    It takes a string, then parse it.\n",
    "    Finally, it retuns a soup object.\n",
    "    \"\"\"\n",
    "    soup = bs4.BeautifulSoup(to_parse, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to collect all sections links\n",
    "# Like world news, national news ...\n",
    "\n",
    "def all_section(main_url):\n",
    "    \"\"\"\n",
    "    It takes a main url of the newspaper and then\n",
    "    finds almost all the sections in the newspaper.\n",
    "    Finally, it returns the section which we will scrap.\n",
    "    \"\"\"\n",
    "    soup = parse_html(request_url(url))\n",
    "    ul = soup.find(\"div\", class_=\"MainLMenu tab\").ul\n",
    "    section_list = []\n",
    "    for li in ul.find_all(\"li\"):\n",
    "        section_list.append(li.a.get('href'))\n",
    "    # Remove the section which we will not consider\n",
    "    # Like the video section and others\n",
    "    remove = [0, 1, -1, -1, -1]\n",
    "    for i in remove:\n",
    "        section_list.remove(section_list[i])\n",
    "    return section_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All section url (half-urls)\n",
    "section_urls = all_section(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/latest-news.html',\n",
       " '/news/national-news-hindi.html',\n",
       " '/feature-news-hindi.html',\n",
       " '/politics-news-hindi.html',\n",
       " '/world-news-hindi.html',\n",
       " '/common-man-issue-news-hindi.html',\n",
       " '/technology-hindi.html',\n",
       " '/business-hindi.html',\n",
       " '/cricket-hindi.html',\n",
       " '/automobile',\n",
       " '/lifestyle-hindi.html']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the sections urls\n",
    "section_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Orientation of the section pages**\n",
    "\n",
    "In this newspaper there are basically two types of layouts: Grid View and List View. So we will filter the section of the grid and list view seperately. As depending upon the layouts the html fromat cahges, therefore we will have to scrap them differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep url of the section which has Grid layout\n",
    "grid_layout_urls = [3, 4, 6, 7, 8, 9]\n",
    "\n",
    "# All Grid half urls\n",
    "grid_urls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Keep url of the section, which has linear layout(List) view\n",
    "list_layout_urls = [0, 1, 5]\n",
    "\n",
    "# All List half urls\n",
    "list_urls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting only Gride View section page urls\n",
    "for filter_url in grid_layout_urls:\n",
    "    grid_urls.append(section_urls[filter_url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting only List View section page urls\n",
    "for filter_url in list_layout_urls:\n",
    "    list_urls.append(section_urls[filter_url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/politics-news-hindi.html',\n",
       " '/world-news-hindi.html',\n",
       " '/technology-hindi.html',\n",
       " '/business-hindi.html',\n",
       " '/cricket-hindi.html',\n",
       " '/automobile']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the Grid View urls\n",
    "grid_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/latest-news.html',\n",
       " '/news/national-news-hindi.html',\n",
       " '/common-man-issue-news-hindi.html']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking List view urls\n",
    "list_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build complete url\n",
    "# Returns a string\n",
    "def complete_url(half_url):\n",
    "    \"\"\"\n",
    "    This takes a second half of the url as input\n",
    "    and then it adds the first part of the url.\n",
    "    Finally it returns a complete url.\n",
    "    \"\"\"\n",
    "   # Join the url with the href of world news\n",
    "    full_url = url + half_url\n",
    "    return full_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To store comple urls of the Grid View\n",
    "final_grid_urls =[]\n",
    "\n",
    "# To store comple urls of the List View\n",
    "final_list_urls =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting half urls to complete urls - Grid View\n",
    "for url_get in grid_urls:\n",
    "    final_grid_urls.append(complete_url(url_get))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting half urls to complete urls - List View\n",
    "for url_set in list_urls:\n",
    "    final_list_urls.append(complete_url(url_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.jagran.com/politics-news-hindi.html',\n",
       " 'https://www.jagran.com/world-news-hindi.html',\n",
       " 'https://www.jagran.com/technology-hindi.html',\n",
       " 'https://www.jagran.com/business-hindi.html',\n",
       " 'https://www.jagran.com/cricket-hindi.html',\n",
       " 'https://www.jagran.com/automobile']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the full urls - Grid View\n",
    "final_grid_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.jagran.com/latest-news.html',\n",
       " 'https://www.jagran.com/news/national-news-hindi.html',\n",
       " 'https://www.jagran.com/common-man-issue-news-hindi.html']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the full urls - List View\n",
    "final_list_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for valid urls\n",
    "def valid_url(url):\n",
    "    \"\"\"\n",
    "    Takes an url and checks if the urls is valid.\n",
    "    Returns a boolearn value.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        urllib2.urlopen(url)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect article uls only for Grid view sections\n",
    "def collect(page_urls):\n",
    "    \"\"\"\n",
    "    Takes a list of urls which has a grid view layout,\n",
    "    then it extracts the urls of the articles from it\n",
    "    and then it returns it.\n",
    "    \"\"\"\n",
    "    print(\"Extracting article urls from the following sections:\")\n",
    "    \n",
    "    all_urls = set()\n",
    "    for page_url in page_urls:\n",
    "        print(page_url)\n",
    "        soup_page = parse_html(request_url(page_url))\n",
    "        for div in soup_page.find_all(class_=\"h3\"):\n",
    "            sec_head_href = div.find(\"a\").get(\"href\")\n",
    "            # Checks if the url is valid\n",
    "            # Add only if the url is valid\n",
    "            if valid_url(sec_head_href):\n",
    "                all_urls.add(sec_head_href)\n",
    "            else:\n",
    "                all_urls.add(complete_url(sec_head_href))\n",
    "    return all_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting article urls from the following sections:\n",
      "https://www.jagran.com/politics-news-hindi.html\n",
      "https://www.jagran.com/world-news-hindi.html\n",
      "https://www.jagran.com/technology-hindi.html\n",
      "https://www.jagran.com/business-hindi.html\n",
      "https://www.jagran.com/cricket-hindi.html\n",
      "https://www.jagran.com/automobile\n"
     ]
    }
   ],
   "source": [
    "# Function call to collect all article urls from Grid View Sections\n",
    "all_urls = collect(final_grid_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of uniques aritcles urls\n",
    "len(all_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
